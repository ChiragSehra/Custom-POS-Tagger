{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Part of Speech Tagger\n",
    "\n",
    "## Introduction:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"files/first.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is Part of Speech?\n",
    "\n",
    "The part of speech explains how a word is used in a sentence. There are eight main parts of speech - nouns, pronouns, adjectives, verbs, adverbs, prepositions, conjunctions and interjections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"files/second.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Noun (N)- Daniel, London, table, dog, teacher, pen, city, happiness, hope\n",
    "2. Verb (V)- go, speak, run, eat, play, live, walk, have, like, are, is\n",
    "3. Adjective(ADJ)- big, happy, green, young, fun, crazy, three\n",
    "4. Adverb(ADV)- slowly, quietly, very, always, never, too, well, tomorrow\n",
    "5. Preposition (P)- at, on, in, from, with, near, between, about, under\n",
    "6. Conjunction (CON)- and, or, but, because, so, yet, unless, since, if\n",
    "7. Pronoun(PRO)- I, you, we, they, he, she, it, me, us, them, him, her, this\n",
    "8. Interjection (INT)- Ouch! Wow! Great! Help! Oh! Hey! Hi!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How does POS tagging work?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"files/third.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Neo', 'NNP'), ('is', 'VBZ'), ('the', 'DT'), ('one', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize, pos_tag\n",
    "\n",
    "print(pos_tag(word_tokenize(\"Neo is the one\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/fourth.jpeg\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "tagged_sentences = nltk.corpus.treebank.tagged_sents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Pierre', 'NNP'), ('Vinken', 'NNP'), (',', ','), ('61', 'CD'), ('years', 'NNS'), ('old', 'JJ'), (',', ','), ('will', 'MD'), ('join', 'VB'), ('the', 'DT'), ('board', 'NN'), ('as', 'IN'), ('a', 'DT'), ('nonexecutive', 'JJ'), ('director', 'NN'), ('Nov.', 'NNP'), ('29', 'CD'), ('.', '.')]\n",
      "Tagged sentences:  3914\n",
      "Tagged words:  100676\n"
     ]
    }
   ],
   "source": [
    "print(tagged_sentences[0])\n",
    "print(\"Tagged sentences: \", len(tagged_sentences))\n",
    "print(\"Tagged words: \", len(nltk.corpus.treebank.tagged_words()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def features(sentence, index):\n",
    "    \"\"\" sentence : [w1,w2,....], index: the index of word\"\"\"\n",
    "    return {\n",
    "        'word': sentence[index],\n",
    "        'is_first': index == 0,\n",
    "        'is_last' : index == len(sentence) -1,\n",
    "        'is_captitalized': sentence[index][0].upper() == sentence[index][0],\n",
    "        'is_all_caps': sentence[index].upper() == sentence[index],\n",
    "        'is_all_lower': sentence[index].lower() == sentence[index],\n",
    "        'prefix_1': sentence[index][0],\n",
    "        'prefix_2':sentence[index][:2],\n",
    "        'prefix_3':sentence[index][:3],\n",
    "        'suffix_1':sentence[index][-1],\n",
    "        'suffix_2':sentence[index][-2:],\n",
    "        'suffix_3':sentence[index][-3:],\n",
    "        'prev_word': '' if index ==0 else sentence[index -1],\n",
    "        'next_word':'' if index == len(sentence) -1 else sentence[index+1],\n",
    "        'has_hyphen':'-' in sentence[index],\n",
    "        'is_numeric':sentence[index].isdigit(),\n",
    "        'capitals_inside':sentence[index][1:].lower() != sentence[index][1:]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'capitals_inside': True,\n",
      " 'has_hyphen': False,\n",
      " 'is_all_caps': True,\n",
      " 'is_all_lower': False,\n",
      " 'is_captitalized': True,\n",
      " 'is_first': False,\n",
      " 'is_last': False,\n",
      " 'is_numeric': False,\n",
      " 'next_word': 'in',\n",
      " 'prefix_1': 'D',\n",
      " 'prefix_2': 'DI',\n",
      " 'prefix_3': 'DIE',\n",
      " 'prev_word': 'America',\n",
      " 'suffix_1': 'S',\n",
      " 'suffix_2': 'ES',\n",
      " 'suffix_3': 'IES',\n",
      " 'word': 'DIES'}\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "pprint.pprint(features(['Captain','America','DIES','in','Infinity','War'],2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def untag(tagged_sentences):\n",
    "    return [w for w, t in tagged_sentences]\n",
    "\n",
    "# for untagging the tags that are associated with our tagged corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2935\n",
      "979\n"
     ]
    }
   ],
   "source": [
    "# Spliting the dataset for training and testing\n",
    "cutoff = int(0.75 * len(tagged_sentences))\n",
    "training_sentences = tagged_sentences[:cutoff]\n",
    "test_sentences = tagged_sentences[cutoff:]\n",
    "\n",
    "print(len(training_sentences))\n",
    "print(len(test_sentences))\n",
    "\n",
    "def transform_to_dataset(tagged_sentences):\n",
    "    X, y = [],[]\n",
    "    \n",
    "    for tagged in tagged_sentences:\n",
    "        for index in range(len(tagged)):\n",
    "            X.append(features(untag(tagged),index))\n",
    "            y.append(tagged[index][1])\n",
    "            \n",
    "    return X,y\n",
    "\n",
    "X,y = transform_to_dataset(training_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Completed\n",
      "Accuracy:  0.8934597461031657\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "clf = Pipeline([\n",
    "    ('vectorizer', DictVectorizer(sparse=False)),\n",
    "    ('classifier', DecisionTreeClassifier(criterion='entropy'))\n",
    "])\n",
    "\n",
    "clf.fit(X[:10000], y[:10000])\n",
    "print('Training Completed')\n",
    "\n",
    "X_test, y_test = transform_to_dataset(test_sentences)\n",
    "\n",
    "print(\"Accuracy: \",clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Mr.', 'NNP'], ['Stark', 'NNP'], ['is', 'VBZ'], ['going', 'VBG'], ['to', 'TO'], ['create', 'VB'], ['another', 'DT'], ['infinity', 'NN'], ['stone', 'NN']]\n"
     ]
    }
   ],
   "source": [
    "def pos_tag(sentence):\n",
    "    tags = clf.predict([features(sentence, index) for index in range(len(sentence))])\n",
    "    tagged_sentence = list(map(list,zip(sentence,tags)))\n",
    "    return tagged_sentence\n",
    "\n",
    "print(pos_tag(word_tokenize('Mr. Stark is going to create another infinity stone')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['thank', 'IN'],\n",
       " ['you', 'PRP'],\n",
       " ['for', 'IN'],\n",
       " ['calling', 'VBG'],\n",
       " ['rayos_no_hassle_copay', 'NN'],\n",
       " ['program', 'NN']]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_tag(word_tokenize('thank you for calling rayos_no_hassle_copay program'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package tagsets to\n",
      "[nltk_data]     C:\\Users\\chirag\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package tagsets is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('tagsets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO VIEW TAGGED CORPORA\n",
    "#  C:\\Users\\chirag\\AppData\\Roaming\\nltk_data\\corpora\\treebank\\tagged> \n",
    "\n",
    "# NLTK BOOK\n",
    "# http://www.nltk.org/book/ch05.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
